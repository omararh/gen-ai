{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement RAG chunking strategies with LangChain and Mistral API\n",
    "\n",
    " The overall goal will be to perform chunking to effective implement.\n",
    "\n",
    "Some key components of chunking include: \n",
    "* Chunking strategy: Choosing the right chunking strategy for your RAG application is important as it determines the boundaries for setting chunks. We will explore some of these in the next section. \n",
    "* Chunk size: Maximum number of tokens to be in each chunk. Determining the appropriate chunk size usually involves some experimenting. \n",
    "* Chunk overlap: The number of tokens overlapping between chunks to preserve context. This is an optional parameter.\n",
    "\n",
    "\n",
    "\n",
    "## Choosing the right chunking strategy for your RAG application\n",
    "There are several different chunking strategies to choose from. It is important to select the most effective chunking technique for the specific use case of your LLM application. Some commonly used chunking processes include:\n",
    "\n",
    "\n",
    "* **Fixed-size chunking**: Splitting text based on a chunk size and optional chunk overlap. This approach is most common and straightforward.\n",
    "\n",
    "* **Recursive chunking**: Iterating default separators until one of them produces the preferred chunk size. Default separators include `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This chunking method uses hierarchical separators so that paragraphs, followed by sentences and then words, are kept together as much as possible.  \n",
    "\n",
    "* **Semantic chunking**: Splitting text in a way that groups sentences based on the semantic similarity of their. Embeddings of high semantic similarity are closer together than those of low semantic similarity. This results in context-aware chunks.\n",
    "\n",
    "* **Document-based chunking**: Splitting based on document structure. This splitter can utilize Markdown text, images, tables and even Python code classes and functions as ways of determining structure. In doing so, large documents can be chunked and processed by the LLM.\n",
    "\n",
    "* **Agentic chunking**: Leverages [agentic AI](https://www.ibm.com/think/topics/ai-agents) by allowing the LLM to determine appropriate document splitting based on semantic meaning as well as content structure such as paragraph types, section headings, step-by-step instructions and more. This chunker is experimental and attempts to simulate human reasoning when processing long documents.\n",
    "\n",
    "-> We will focus on **Semantic chunking** because it is considered as the best strategy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T18:36:50.772166Z",
     "start_time": "2025-09-20T18:34:27.845945Z"
    }
   },
   "source": "!pip install -q langchain mistralai langchain_experimental langchain-text-splitters langchain_chroma transformers bs4 langchain_huggingface sentence-transformers",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# imports \n",
    "import getpass\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T18:38:34.562036Z",
     "start_time": "2025-09-20T18:38:25.238868Z"
    }
   },
   "source": "MISTRAL_APIKEY = getpass.getpass(\"Please enter your Mistral API key (hit enter): \")",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install langchain-mistralai\n",
    "from langchain_mistralai import ChatMistralAI # Normal Mistral SDK is not compatible with `langchain`"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 4. Initialize your LLM\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:06:36.340163Z",
     "start_time": "2025-09-20T20:06:36.332064Z"
    }
   },
   "source": "llm = ChatMistralAI(model=\"mistral-small-latest\", mistral_api_key=MISTRAL_APIKEY)",
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Load your document\n",
    "\n",
    "The context we are using for our RAG pipeline is the official IBM announcement for the release of Granite 3.1. We can load the blog to a `Document` directly from the webpage by using LangChain's `WebBaseLoader`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T18:45:42.259915Z",
     "start_time": "2025-09-20T18:45:41.946064Z"
    }
   },
   "source": [
    "url = \"https://www.ibm.com/new/announcements/ibm-granite-3-1-powerful-performance-long-context-and-more\"\n",
    "doc = WebBaseLoader(url).load()"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic chunking\n",
    "Semantic chunking requires an embedding or encoder model. We can use the `granite-embedding-30m-english` model as our embedding model. We can also print one of the chunks for a better understanding of their structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"ibm-granite/granite-embedding-30m-english\")\n",
    "text_splitter = SemanticChunker(embeddings_model)\n",
    "semantic_chunks = text_splitter.create_documents([doc[0].page_content])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Create vector store\n",
    "\n",
    "Now that we have experimented with various chunking strategies, let's move along with our RAG implementation. For this tutorial, we will choose the chunks produced by the semantic split and convert them to vector embeddings. An open source vector store we can use is [Chroma DB](https://python.langchain.com/docs/integrations/vectorstores/chroma/). We can easily access Chroma functionality through the `langchain_chroma` package.\n",
    "\n",
    "Let's initialize our Chroma vector database, provide it with our embeddings model and add our documents produced by semantic chunking."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T13:39:41.937458Z",
     "start_time": "2025-09-21T13:39:41.918674Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "vector_db = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=os.path.abspath(\"../chroma_langchain_db\"),  # Where to save data locally, remove if not necessary\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T13:39:43.861058Z",
     "start_time": "2025-09-21T13:39:43.520127Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "# Nettoyer les \\n et espaces multiples dans les chunks\n",
    "for chunk in semantic_chunks:\n",
    "    # Remplace un ou plusieurs \\n par un seul espace\n",
    "    chunk.page_content = re.sub(r'\\n+', ' ', chunk.page_content)\n",
    "    # Remplace plusieurs espaces consécutifs par un seul espace\n",
    "    chunk.page_content = re.sub(r'\\s+', ' ', chunk.page_content).strip()\n",
    "\n",
    "# Puis ajouter à la base\n",
    "vector_db.add_documents(semantic_chunks)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6f0092e9-5390-456f-8cd5-0baed4c68923',\n",
       " 'f9da10c8-f264-4299-90c0-5694c4fcf2ca',\n",
       " '8f96f844-0bd6-4e02-993e-cf1306264683',\n",
       " '179da3fc-dd37-40ae-9e2a-79e6a7c5f607',\n",
       " '167ca692-e8d5-4edf-a0b6-944ec89ce88f']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Structure the prompt template\n",
    "Next, we can move onto creating a prompt template for our LLM. This prompt template allows us to ask multiple questions without altering the initial prompt structure. We can also provide our vector store as the retriever. This step finalizes the RAG structure. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:07:28.593059Z",
     "start_time": "2025-09-20T20:07:28.587535Z"
    }
   },
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt_template = \"\"\"<|start_of_role|>user<|end_of_role|>Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {input}<|end_of_text|>\n",
    "<|start_of_role|>assistant<|end_of_role|>\"\"\"\n",
    "\n",
    "qa_chain_prompt = PromptTemplate.from_template(prompt_template)\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, qa_chain_prompt)\n",
    "rag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain)"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. Prompt the RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our completed RAG workflow, let's invoke a user query. First, we can strategically prompt the model without any additional context from the vector store we built to test whether the model is using its built-in knowledge or truly using the RAG context. The Granite 3.1 announcement blog references [Docling](https://github.com/DS4SD/docling?tab=readme-ov-file), IBM's tool for parsing various document types and converting them into Markdown or JSON. Let's ask the LLM about Docling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the model was not trained on information about Docling and without outside tools or information, it cannot provide us with the correct information. The model hallucinates. Now, let's try providing the same query to the RAG chain we built. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:08:28.423356Z",
     "start_time": "2025-09-20T20:08:24.493287Z"
    }
   },
   "source": [
    "rag_output = rag_chain.invoke({\"input\": \"What is Docling?\"})\n",
    "rag_output['answer']"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Docling is an open-source tool developed by IBM for preprocessing and extracting information from various document formats, making them more accessible for large language models (LLMs) like Granite. It can parse documents in formats such as PDF, DOCX, images, PPTX, XLSX, HTML, and AsciiDoc, converting them into model-friendly formats like Markdown or JSON. This enables the information to be easily accessed by models for tasks like retrieval-augmented generation (RAG) and other workflows.\\n\\nDocling goes beyond simple optical character recognition (OCR) and text extraction by integrating contextual and element-based preprocessing techniques. For example, it can extract tables spanning multiple pages as a single table or separate body text, images, and tables based on their original context. The tool is designed to work seamlessly with agentic frameworks like LlamaIndex, LangChain, and Bee, and is open-sourced under the MIT License.\\n\\nAdditional features under development include equation and code extraction, as well as metadata extraction. IBM provides tutorials and resources for using Docling in conjunction with Granite models, such as building a document question-answering system.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The Granite model correctly used the RAG context to tell us correct information about Docling while preserving semantic coherence. We proved this same result was not possible without the use of RAG. \n",
    "\n",
    "## Summary\n",
    "In this tutorial, you created a RAG pipeline and experimented with several chunking strategies to improve the system’s retrieval accuracy. Using the Granite 3.1 model, we successfully produced appropriate model responses to a user query related to the documents provided as context. The text we used for this RAG implementation was loaded from a blog on ibm.com announcing the release of Granite 3.1. The model provided us with information only accessible through the provided context since it was not part of the model's initial knowledge base. \n",
    "\n",
    "For those in search of further reading, check out the results of a [project](https://developer.ibm.com/articles/awb-enhancing-llm-performance-document-chunking-with-watsonx/) comparing LLM performance using HTML structured chunking in comparison to watsonx chunking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
